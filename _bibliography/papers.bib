---
---
@article{padurean2024benchmarking,
abbr      = {NeurIPS D\&B},
title     = {{B}enchmarking {G}enerative {M}odels on {C}omputational {T}hinking {T}ests in {E}lementary {V}isual {P}rogramming},
author    = {Victor-Alexandru Padurean and Adish Singla},
journal   = {Proceedings of the Annual Conference on Neural Information Processing Systems ({NeurIPS}) Track on Datasets and Benchmarks},
year      = {2024},
paper     = {https://arxiv.org/abs/2406.09891},
arxiv     = {https://arxiv.org/abs/2406.09891},
abstract  = {Generative models have demonstrated human-level proficiency in various benchmarks across domains like programming, natural sciences, and general knowledge. Despite these promising results on competitive benchmarks, they still struggle with seemingly simple problem-solving tasks typically carried out by elementary-level students. How do state-of-the-art models perform on standardized tests designed to assess computational thinking and problem-solving skills at schools? In this paper, we curate a novel benchmark involving computational thinking tests grounded in elementary visual programming domains. Our initial results show that state-of-the-art models like GPT-4o and Llama3 barely match the performance of an average school student. To further boost the performance of these models, we fine-tune them using a novel synthetic data generation methodology. The key idea is to develop a comprehensive dataset using symbolic methods that capture different skill levels, ranging from recognition of visual elements to multi-choice quizzes to synthesis-style tasks. We showcase how various aspects of symbolic information in synthetic data help improve fine-tuned models' performance. We will release the full implementation and datasets to facilitate further research on enhancing computational thinking in generative models.},
selected  = {true}
}

@article{padurean2025bugspotter,
abbr      = {SIGCSE},
title     = {{BugSpotter}: {A}utomated {G}eneration of {C}ode {D}ebugging {E}xercises},
author    = {Victor-Alexandru Padurean and Paul Denny and Adish Singla},
journal   = {Proceedings of the Technical Symposium on Computer Science Education {(SIGCSE)}},
year      = {2025},
note      = {to appear},
abstract  = {Debugging is an essential yet often under-emphasized skill in programming education. In the era of code-generating large language models (LLMs), the ability for students to reason about code and identify errors is increasingly important. However, students frequently resort to trial-and-error methods to resolve bugs without truly understanding the underlying issues. Developing the ability to identify and hypothesize the cause of bugs is crucial but can be time-consuming to teach effectively through traditional means. This paper introduces BugSpotter, an innovative tool that leverages an LLM to generate buggy code from a problem description and verifies the synthesized bugs via a test suite. Students interact with BugSpotter by designing failing test cases, where the buggy code's output differs from the expected result as defined by the problem specification. This not only provides opportunities for students to enhance their debugging skills, but also to practice reading and understanding problem specifications. We deployed BugSpotter in a large classroom setting and compared the debugging exercises it generated to exercises hand-crafted by an instructor for the same problems. We found that the LLM-generated exercises produced by BugSpotter varied in difficulty and were well-matched to the problem specifications. Importantly, the LLM-generated exercises were comparable to those manually created by instructors with respect to student performance, suggesting that BugSpotter could be an effective and efficient aid for learning debugging.},
selected  = {true}
}

@article{padurean2024neural,
abbr      = {TMLR},
title     = {{N}eural {T}ask {S}ynthesis for {V}isual {P}rogramming},
author    = {Victor-Alexandru Padurean and Georgios Tzannetos and Adish Singla},
journal   = {Transactions on Machine Learning Research ({TMLR})},
year      = {2024},
paper     = {https://arxiv.org/abs/2305.18342},
arxiv     = {https://arxiv.org/abs/2305.18342},
code      = {https://github.com/machine-teaching-group/tmlr2024_neurtasksyn},
abstract  = {Generative neural models hold great promise in enhancing programming education by synthesizing new content. We seek to design neural models that can automatically generate programming tasks for a given specification in the context of visual programming domains. Despite the recent successes of large generative models like GPT-4, our initial results show that these models are ineffective in synthesizing visual programming tasks and struggle with logical and spatial reasoning. We propose a novel neuro-symbolic technique, NeurTaskSyn, that can synthesize programming tasks for a specification given in the form of desired programming concepts exercised by its solution code and constraints on the visual task. NeurTaskSyn has two components: the first component is trained via imitation learning procedure to generate possible solution codes, and the second component is trained via reinforcement learning procedure to guide an underlying symbolic execution engine that generates visual tasks for these codes. We demonstrate the effectiveness of NeurTaskSyn through an extensive empirical evaluation and a qualitative study on reference tasks taken from the Hour of Code: Classic Maze challenge by Code.org and the Intro to Programming with Karel course by CodeHS.com.},
selected  = {true}
}

@article{DBLP:conf/lak/PhungPS0CGSS24,
  abbr          = {LAK},
  author        = {Tung Phung and
                  Victor-Alexandru Padurean and
                  Anjali Singh and
                  Christopher Brooks and
                  Jos{\'{e}} Cambronero and
                  Sumit Gulwani and
                  Adish Singla and
                  Gustavo Soares},
  title         = {{A}utomating {H}uman {T}utor-{S}tyle {P}rogramming {F}eedback: {L}everaging {GPT-4}
                  {T}utor {M}odel for {H}int {G}eneration and {GPT-3.5} {S}tudent {M}odel for {H}int
                  {V}alidation},
  journal     = {Proceedings of the Learning Analytics and Knowledge Conference ({LAK})},
  year          = {2024},
  paper         = {https://dl.acm.org/doi/10.1145/3636555.3636846},
  arxiv         = {https://arxiv.org/abs/2310.03780},
  abstract      = {Generative AI and large language models hold great promise in enhancing programming education by automatically generating individualized feedback for students. We investigate the role of generative AI models in providing human tutor-style programming hints to help students resolve errors in their buggy programs. Recent works have benchmarked state-of-the-art models for various feedback generation scenarios; however, their overall quality is still inferior to human tutors and not yet ready for real-world deployment. In this paper, we seek to push the limits of generative AI models toward providing high-quality programming hints and develop a novel technique, GPT4Hints-GPT3.5Val. As a first step, our technique leverages GPT-4 as a ``tutor'' model to generate hints -- it boosts the generative quality by using symbolic information of failing test cases and fixes in prompts. As a next step, our technique leverages GPT-3.5, a weaker model, as a ``student'' model to further validate the hint quality -- it performs an automatic quality validation by simulating the potential utility of providing this feedback. We show the efficacy of our technique via extensive evaluation using three real-world datasets of Python programs covering a variety of concepts ranging from basic algorithms to regular expressions and data analysis using pandas library.},
}

@article{DBLP:journals/corr/abs-2306-17156,
  abbr          = {ICER},
  author       = {Tung Phung and
                  Victor-Alexandru Padurean and
                  Jos{\'{e}} Cambronero and
                  Sumit Gulwani and
                  Tobias Kohn and
                  Rupak Majumdar and
                  Adish Singla and
                  Gustavo Soares},
  title         = {{G}enerative {AI} for {P}rogramming {E}ducation: {B}enchmarking {C}hatGPT, {G}PT-4, and {H}uman {T}utors},
  journal       = {Proceedings of the Conference on International Computing Education Research ({ICER}) - Volume 2},
  year          = {2023},
  paper         = {https://arxiv.org/abs/2306.17156},
  arxiv         = {https://arxiv.org/abs/2306.17156},
  abstract      = {Generative AI and large language models hold great promise in enhancing computing education by powering next-generation educational technologies for introductory programming. Recent works have studied these models for different scenarios relevant to programming education; however, these works are limited for several reasons, as they typically consider already outdated models or only specific scenario(s). Consequently, there is a lack of a systematic study that benchmarks state-of-the-art models for a comprehensive set of programming education scenarios. In our work, we systematically evaluate two models, ChatGPT (based on GPT-3.5) and GPT-4, and compare their performance with human tutors for a variety of scenarios. We evaluate using five introductory Python programming problems and real-world buggy programs from an online platform, and assess performance using expert-based annotations. Our results show that GPT-4 drastically outperforms ChatGPT (based on GPT-3.5) and comes close to human tutors' performance for several scenarios. These results also highlight settings where GPT-4 still struggles, providing exciting future directions on developing techniques to improve the performance of these models.}
}